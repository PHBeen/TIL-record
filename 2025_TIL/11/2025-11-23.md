# TIL Template
날짜: 2025-11-23

## 스크럼
- 학습 목표 1 : transformer 모델 직접 구현하기.
- 학습 목표 2 : transformer 각 layer의 텐서 및 데이터 분포 확인하기.
- 학습 목표 3 : GPT를 통해 학습데이터 준비하기


## 새로 배운 내용
### 주제 1:  transformer 모델 직접 구현하기.
- **Bert 모델의 구조**: 
  1. Bert는 Bidirectional Encoder Representations from Transformers를 의미함.
  2. Token Embedding, Position Embedding을 사용해서 문장을 표현하고, Token Embedding중 CLS(Special Classification Token)은 문장의 첫번째 토큰에 삽입되어 문장의 시작을 알리는 역할을 하며 SEP(Special Separator token)은 각 문장을 구별하는 역할을 한다. Position Embedding은 토큰의 위치를 알려주는 임베딩이다.
  3. MLM, NSP
  ```
    ┌────────────────────────────────────────────────────────────┐
    │                        BERT MODEL                           │
    └────────────────────────────────────────────────────────────┘

    INPUT (Token IDs, Segment IDs, Position IDs)
                │
                ▼
    ┌────────────────────────────────────────────────────────────┐
    │                     0. EMBEDDINGS                          │
    │  Token Embedding + Segment Embedding + Position Embedding   │
    │  → LayerNorm → Dropout                                      │
    └────────────────────────────────────────────────────────────┘
                │
                ▼
    ┌────────────────────────────────────────────────────────────┐
    │         1~12. TRANSFORMER ENCODER LAYER (×12)               │
    │                                                            │
    │   ┌────────────────────────────────────────────────────┐    │
    │   │                Multi-Head Self-Attention            │    │
    │   │  Q = XWq                                           │    │
    │   │  K = XWk                                           │    │
    │   │  V = XWv                                           │    │
    │   │  Attention = softmax(QKᵀ / √d) V                   │    │
    │   └────────────────────────────────────────────────────┘    │
    │                 │ (Residual + LayerNorm)                   │
    │                 ▼                                           │
    │   ┌────────────────────────────────────────────────────┐    │
    │   │         FFN (MLP: 768 → 3072 → 768, GeLU)          │    │
    │   └────────────────────────────────────────────────────┘    │
    │                 │ (Residual + LayerNorm)                   │
    └────────────────────────────────────────────────────────────┘
                │
                ▼
    ┌────────────────────────────────────────────────────────────┐
    │                       OUTPUT                                │
    │   - Per-token contextualized embeddings (seq_len × 768)     │
    │   - [CLS] vector (768) → 문장 분류, NLI 등 사용             │
    └────────────────────────────────────────────────────────────┘
    ```

- **BERT 모델 구현하기**:  Colab 코드는 링크 참고.
<img width="1720" height="353" alt="image" src="https://github.com/user-attachments/assets/4798d27b-32af-47be-a855-d5e69a48d46d" />


### 주제 2: transformer 각 layer의 텐서 및 데이터 분포 확인하기




### 주제 3: GPT를 통해 학습데이터 준비하기


## 오늘의 도전 과제와 해결 방법


## 오늘의 회고


## 참고 자료 및 링크
- [BERT 모델 구현 Colab 링크](https://colab.research.google.com/drive/14wDKk9xsRz2ke8VouwsgefUWjwb8l7DV?usp=sharing)

