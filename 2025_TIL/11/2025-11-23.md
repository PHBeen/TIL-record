# TIL Template
날짜: 2025-11-23

## 스크럼
- 학습 목표 1 : transformer 모델 직접 구현하기.
- 학습 목표 2 : transformer 각 layer의 텐서 및 데이터 분포 확인하기.
- 학습 목표 3 : GPT를 통해 학습데이터 준비하기


## 새로 배운 내용
### 주제 1:  transformer 모델 직접 구현하기.
- **Bert 모델의 구조**: 
  1. Bert는 Bidirectional Encoder Representations from Transformers를 의미함.
  2. Token Embedding, Position Embedding을 사용해서 문장을 표현하고, Token Embedding중 CLS(Special Classification Token)은 문장의 첫번째 토큰에 삽입되어 문장의 시작을 알리는 역할을 하며 SEP(Special Separator token)은 각 문장을 구별하는 역할을 한다. Position Embedding은 토큰의 위치를 알려주는 임베딩이다.
  3. MLM, NSP
  ```
    ┌────────────────────────────────────────────────────────────┐
    │                        BERT MODEL                           │
    └────────────────────────────────────────────────────────────┘

    INPUT (Token IDs, Segment IDs, Position IDs)
                │
                ▼
    ┌────────────────────────────────────────────────────────────┐
    │                     0. EMBEDDINGS                          │
    │  Token Embedding + Segment Embedding + Position Embedding   │
    │  → LayerNorm → Dropout                                      │
    └────────────────────────────────────────────────────────────┘
                │
                ▼
    ┌────────────────────────────────────────────────────────────┐
    │         1~12. TRANSFORMER ENCODER LAYER (×12)               │
    │                                                            │
    │   ┌────────────────────────────────────────────────────┐    │
    │   │                Multi-Head Self-Attention            │    │
    │   │  Q = XWq                                           │    │
    │   │  K = XWk                                           │    │
    │   │  V = XWv                                           │    │
    │   │  Attention = softmax(QKᵀ / √d) V                   │    │
    │   └────────────────────────────────────────────────────┘    │
    │                 │ (Residual + LayerNorm)                   │
    │                 ▼                                           │
    │   ┌────────────────────────────────────────────────────┐    │
    │   │         FFN (MLP: 768 → 3072 → 768, GeLU)          │    │
    │   └────────────────────────────────────────────────────┘    │
    │                 │ (Residual + LayerNorm)                   │
    └────────────────────────────────────────────────────────────┘
                │
                ▼
    ┌────────────────────────────────────────────────────────────┐
    │                       OUTPUT                                │
    │   - Per-token contextualized embeddings (seq_len × 768)     │
    │   - [CLS] vector (768) → 문장 분류, NLI 등 사용             │
    └────────────────────────────────────────────────────────────┘
    ```

- **BERT 모델 구현하기**:  Colab 코드는 링크 참고.

<결과>
<img width="149" height="54" alt="image" src="https://github.com/user-attachments/assets/5b51921e-ff65-4cfc-9e53-0d6fd0d784de" />

3개의 toy데이터는 학습이 제대로 진행됨.(인정, 안정감, 자율성 관련 데이터임)

<img width="1720" height="353" alt="image" src="https://github.com/user-attachments/assets/4798d27b-32af-47be-a855-d5e69a48d46d" />

- Your text에 챗봇처럼 글을 입력하면, 10개의 class에 %단위로 감정을 나타내는 테스트를 진행함.
- 성취에 대한 text를 입력했음에도 성취가 5%미만으로 낮게 측정되는 것을 보아, 모델이 아직 성취를 학습하지 못했음을 의미함. -> 추가 학습이 필요함.


### 주제 2: transformer 각 layer의 텐서 및 데이터 분포 확인하기
- **Q,K,V 층 이해하기**:

<img width="908" height="407" alt="image" src="https://github.com/user-attachments/assets/8c957846-6f66-4217-b111-5e55e9a470da" />

 y축은 Self-attention + FFN이 반복되는 BERT Encoding layer (1 = 1 layer, 2 = 2 layer)
 x축은 hidden dimension. 원래는 768차원이지만, 보기좋게 128차원만 표기함.
 처음 레이어(0) -> 마지막(12) 레이어의 비교: 
   동일한 토큰이 층이 깊어질 수록 복잡한 의미로 변환됨을 의미함.
   토큰의 위치에 따라 강조되는 의미의 축이 달라짐
   레이어가 깊어질수록 문장 전체에 맞게 의미가 계속 재구성된다.
   패턴이 점점 정형화 된다.

| 0 layer | 12 layer |
| -- | -- |
| <img width="500" height="200" alt="image" src="https://github.com/user-attachments/assets/deb8f8a7-4bb7-40ee-adac-f8017dde13a6" /> | <img width="500" height="200" alt="image" src="https://github.com/user-attachments/assets/569d6323-a0c5-4382-b1f4-ae6883ece12e" /> |
|x축은 y가 바라보는 대상단어, y축은 어떤 단어가 보고 있는가를 의미 <br> 대각선이 밝게 보이는 head는 초기레이어에서 자주 나타나며, 모델이 아직 문맥 정보를 덜 가지고 있다는 뜻이며, 자기자신을 우선적으로 본다. <br> 뒤죽박죽 패턴은 다양한 문맥 단서를 수집하는 헤드임. 언어의 이해를 확장하는  head. <br> 한쪽 열 또는 행만 밝은 head는 [cls], [sep]와 같은 특수토큰을 많이 참고하고 있다. | 대부분의 Head가 오른쪽 끝 열에만 집중한다는 것은 SEP, 마지막 토큰을 주목한다는 뜻이다. 문장의 의미를 종합할때, 마지막 토큰 같은 특수 토큰을 많이 참고하기 때문이다.(논문에서도 밝혀진 사실) |
| Head간 패턴이 완전히 다르다는 사실을 알 수 있음. | 0~3layer는 형태소, 구문등의 특징을, 4~8 layer는 단어와 단어 사이의 의존관계를, 9~12는 문장 전체의 의미, CLS 집계를 알 수 있다. |

- **Bert Encoder의 한 층(Transformer Block) 구조**: 아래 구조가 총 12번 반복되는 구조를 가진다.
```
X0
│
├─ Step 1: LayerNorm(0) : (1, 14, 768)
│
├─ Step 2: Multi-Head Self Attention: (1,14,768)*3(K,Q,V) 
│    -> (1,14,12,64)*3 -> softmax (Qx K^T / sqrt(64) -> @ V) -> (1,14,768)
│
├─ Step 3: Dropout : shape 유지
│
├─ Step 4: Residual Add (X0 + AttnOutput)
│    -> H1 : shape 유지
│
├─ Step 5: LayerNorm(1)
│
├─ Step 6: Feed Forward (Linear → GELU → Linear) : (1, 14, 768 ) -> (1, 14, 3072) 
│     -> (1, 14, 768)
│
├─ Step 7: Dropout : shape 유지
│
└─ Step 8: Residual Add (H1 + FFN_output)
     -> Output of this layer

```

### 주제 3: GPT를 통해 학습데이터 준비하기



## 오늘의 도전 과제와 해결 방법


## 오늘의 회고


## 참고 자료 및 링크
- [BERT 모델 구현 Colab 링크](https://colab.research.google.com/drive/14wDKk9xsRz2ke8VouwsgefUWjwb8l7DV?usp=sharing)

