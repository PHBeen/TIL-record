# TIL Template
날짜: 2025-11-23

## 스크럼
- 학습 목표 1 : transformer 모델 직접 구현하기.
- 학습 목표 2 : transformer 각 layer의 텐서 및 데이터 분포 확인하기.
- 학습 목표 3 : GPT를 통해 학습데이터 준비하기


## 새로 배운 내용
### 주제 1:  transformer 모델 직접 구현하기.
- **Bert 모델의 구조**: 
  1. Bert는 Bidirectional Encoder Representations from Transformers를 의미함.
  2. Token Embedding, Position Embedding을 사용해서 문장을 표현하고, Token Embedding중 CLS(Special Classification Token)은 문장의 첫번째 토큰에 삽입되어 문장의 시작을 알리는 역할을 하며 SEP(Special Separator token)은 각 문장을 구별하는 역할을 한다. Position Embedding은 토큰의 위치를 알려주는 임베딩이다.
  3. MLM, NSP
  ```
    ┌────────────────────────────────────────────────────────────┐
    │                        BERT MODEL                           │
    └────────────────────────────────────────────────────────────┘

    INPUT (Token IDs, Segment IDs, Position IDs)
                │
                ▼
    ┌────────────────────────────────────────────────────────────┐
    │                     0. EMBEDDINGS                          │
    │  Token Embedding + Segment Embedding + Position Embedding   │
    │  → LayerNorm → Dropout                                      │
    └────────────────────────────────────────────────────────────┘
                │
                ▼
    ┌────────────────────────────────────────────────────────────┐
    │         1~12. TRANSFORMER ENCODER LAYER (×12)               │
    │                                                            │
    │   ┌────────────────────────────────────────────────────┐    │
    │   │                Multi-Head Self-Attention            │    │
    │   │  Q = XWq                                           │    │
    │   │  K = XWk                                           │    │
    │   │  V = XWv                                           │    │
    │   │  Attention = softmax(QKᵀ / √d) V                   │    │
    │   └────────────────────────────────────────────────────┘    │
    │                 │ (Residual + LayerNorm)                   │
    │                 ▼                                           │
    │   ┌────────────────────────────────────────────────────┐    │
    │   │         FFN (MLP: 768 → 3072 → 768, GeLU)          │    │
    │   └────────────────────────────────────────────────────┘    │
    │                 │ (Residual + LayerNorm)                   │
    └────────────────────────────────────────────────────────────┘
                │
                ▼
    ┌────────────────────────────────────────────────────────────┐
    │                       OUTPUT                                │
    │   - Per-token contextualized embeddings (seq_len × 768)     │
    │   - [CLS] vector (768) → 문장 분류, NLI 등 사용             │
    └────────────────────────────────────────────────────────────┘
    ```

- **BERT 모델 구현하기**:  Colab 코드는 링크 참고.

<결과>
<img width="149" height="54" alt="image" src="https://github.com/user-attachments/assets/5b51921e-ff65-4cfc-9e53-0d6fd0d784de" />

3개의 toy데이터는 학습이 제대로 진행됨.(인정, 안정감, 자율성 관련 데이터임)

<img width="1720" height="353" alt="image" src="https://github.com/user-attachments/assets/4798d27b-32af-47be-a855-d5e69a48d46d" />

- Your text에 챗봇처럼 글을 입력하면, 10개의 class에 %단위로 감정을 나타내는 테스트를 진행함.
- 성취에 대한 text를 입력했음에도 성취가 5%미만으로 낮게 측정되는 것을 보아, 모델이 아직 성취를 학습하지 못했음을 의미함. -> 추가 학습이 필요함.


### 주제 2: transformer 각 layer의 텐서 및 데이터 분포 확인하기
- **Q,K,V 층 이해하기**:

<img width="908" height="407" alt="image" src="https://github.com/user-attachments/assets/8c957846-6f66-4217-b111-5e55e9a470da" />

 y축은 Self-attention + FFN이 반복되는 BERT Encoding layer (1 = 1 layer, 2 = 2 layer)
 x축은 hidden dimension. 원래는 768차원이지만, 보기좋게 128차원만 표기함.
 처음 레이어(0) -> 마지막(12) 레이어의 비교: 
   동일한 토큰이 층이 깊어질 수록 복잡한 의미로 변환됨을 의미함.
   토큰의 위치에 따라 강조되는 의미의 축이 달라짐
   레이어가 깊어질수록 문장 전체에 맞게 의미가 계속 재구성된다.
   패턴이 점점 정형화 된다.

| 0 layer | 12 layer |
| -- | -- |
| <img width="500" height="200" alt="image" src="https://github.com/user-attachments/assets/deb8f8a7-4bb7-40ee-adac-f8017dde13a6" /> | <img width="500" height="200" alt="image" src="https://github.com/user-attachments/assets/569d6323-a0c5-4382-b1f4-ae6883ece12e" /> |
|x축은 y가 바라보는 대상단어, y축은 어떤 단어가 보고 있는가를 의미 <br> 대각선이 밝게 보이는 head는 초기레이어에서 자주 나타나며, 모델이 아직 문맥 정보를 덜 가지고 있다는 뜻이며, 자기자신을 우선적으로 본다. <br> 뒤죽박죽 패턴은 다양한 문맥 단서를 수집하는 헤드임. 언어의 이해를 확장하는  head. <br> 한쪽 열 또는 행만 밝은 head는 [cls], [sep]와 같은 특수토큰을 많이 참고하고 있다. | 대부분의 Head가 오른쪽 끝 열에만 집중한다는 것은 SEP, 마지막 토큰을 주목한다는 뜻이다. 문장의 의미를 종합할때, 마지막 토큰 같은 특수 토큰을 많이 참고하기 때문이다.(논문에서도 밝혀진 사실) |
| Head간 패턴이 완전히 다르다는 사실을 알 수 있음. | 0~3layer는 형태소, 구문등의 특징을, 4~8 layer는 단어와 단어 사이의 의존관계를, 9~12는 문장 전체의 의미, CLS 집계를 알 수 있다. |

- **Bert Encoder의 한 층(Transformer Block) 구조**: 아래 구조가 총 12번 반복되는 구조를 가진다.
```
X0
│
├─ Step 1: LayerNorm(0) : (1, 14, 768)
│
├─ Step 2: Multi-Head Self Attention: (1,14,768)*3(K,Q,V) 
│    -> (1,14,12,64)*3 -> softmax (Qx K^T / sqrt(64) -> @ V) -> (1,14,768)
│
├─ Step 3: Dropout : shape 유지
│
├─ Step 4: Residual Add (X0 + AttnOutput)
│    -> H1 : shape 유지
│
├─ Step 5: LayerNorm(1)
│
├─ Step 6: Feed Forward (Linear → GELU → Linear) : (1, 14, 768 ) -> (1, 14, 3072) 
│     -> (1, 14, 768)
│
├─ Step 7: Dropout : shape 유지
│
└─ Step 8: Residual Add (H1 + FFN_output)
     -> Output of this layer

```

### 주제 3: GPT를 통해 학습데이터 준비하기
- **데이터 형태 스케치**:
```
{
  "text": "요즘 사람 만나는 게 너무 피곤해... 나 혼자 있고 싶어.",
  "label": [0.6, 0.2, 0.0, 0.0, 0.2, ...]  # 10차원 분포
}
```
label 길이: 10.
0~1의 실수값.
여러 욕구가 동시에 존재 가능하도록 설계.
text는 20~200자의 자연스러운 문장으로 설계.

- **데이터 개수, 파인튜닝 방법, 데이터 저장 형태**: 
 데이터 수 고찰:
  데이터 1000개: 아주 빠른 베이스 확립
  데이터 3000~ 5000개: 각 욕구의 의미 다양성을 확립가능함
  데이터 1만개: 사람레벨의 감정 분포 추정이 가능함.

 파인튜닝 방법 고찰:
  부분 파인튜닝: low level의 의미를 담당하는 0~6layer까지는 Freeze, 감정, 욕구등을 파악하는 7~11 layer와 FC layer는 학습 진행 예정.

 label이 10차원이므로 BCEWithLogitsLoss가 적합할 것임. 

 데이터 저장 형태: .jsonl 형태 채택(line-by-line 로딩이 빠르고, 대규모 데이터 처리에 최적함. PyTorch에서 읽어올 수 있으며 버전관리 쉬움.)
 ```
 {"text": "요즘 사람 만나는 게 너무 피곤해...", "label": [0.6,0.2,0.0,0.0,0.2,0.1,0.0,0.0,0.4,0.1]}
{"text": "승진하고 싶은데 자신이 없어.", "label": [0.3,0.4,0.1,0.85,0.2,0.2,0.5,0.1,0.2,0.3]}
{"text": "혼자 있고 싶어.", "label": [0.6,0.1,0.2,0.05,0.1,0.05,0.1,0.0,0.7,0.05]}

 ```

## 오늘의 도전 과제와 해결 방법
- transformer 모델의 계산과정을 따라가며 계산 과정을 이해한다. Pytorch를 통한 코드 설계 -> 0 layer Head 12개 plot -> 12 layer 12개 plot으로 데이터 내부를 Heatmap으로 확인


## 오늘의 회고
- transformer 모델은 단어의 여러 의미를 해석하도록 만든 모델이다.
- 직접 Heatmap을 출력하며 어떤 단어와 어떤 단어가 연관이 있는지 계산하는 과정을 보며 재미를 느꼈다.
- 학습데이터에 대한 대략적인 계획을 세웠으며 내일은 파인튜닝 후, 서비스화를 진행해야겠다.


## 참고 자료 및 링크
- [BERT 모델 구현 Colab 링크](https://colab.research.google.com/drive/14wDKk9xsRz2ke8VouwsgefUWjwb8l7DV?usp=sharing)

