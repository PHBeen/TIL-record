# TIL Template
날짜: 2025-11-24

## 스크럼
- 학습 목표 1 : profile의 readme.md 작성하기
- 학습 목표 2 : 학습 데이터 생성하기
- 학습 목표 3 : ONNX 추론 최적화 공부하기
- 학습 목표 4: Bert 모델 블로그에 업로드하기


## 새로 배운 내용
### 주제 1: profile의 readme.md 작성하기.
- **readme.md 생성방법**: 이름과 동일한 repo생성하기 -> readme.md에서 나를 설명하는 글 3개만 작성하기.


### 주제 2: 학습 데이터 생성하기, 학습하기.
- **데이터 생성하기**: GPT API를 활용하여 5000개의 데이터셋을 생성하는 자동화 파이프라인을 구축함.
  1. GPT(gpt-4.1-mini) API 생성하기
  2. prompt 작성하기
  3. 50개 데이터셋을 단위로 100번 생성하여 .jsonl 파일에 저장함.


### 주제 3: ONNX 추론 최적화 공부하기.
- **ONNX 추론 최적화 코드, INT8 양자화 코드**:
```
import torch

model = MyEmotionModel()   # 너의 fine-tuned 모델
model.eval()

dummy_input = tokenizer("요즘 힘들어", return_tensors="pt")["input_ids"]

torch.onnx.export(
    model,
    (dummy_input,),             # tuple로 입력
    "emotion_model.onnx",
    input_names=["input_ids"],
    output_names=["logits"],
    opset_version=17,
    dynamic_axes={
        "input_ids": {0: "batch", 1: "sequence"},
        "logits": {0: "batch"}
    }
)
```

```
from onnxruntime.quantization import quantize_dynamic, QuantType

quantize_dynamic(
    "emotion_model.onnx",
    "emotion_model_int8.onnx",
    weight_type=QuantType.QInt8
)

```
- **ONNX를 진행하는 이유**: 
  1. 모델의 속도향상: Python -> C++기반 엔진으로 변경, fp16, int8같은 저정밀 연산도 안정적으로 지원.
  2. 비용 절감: 모델 용량, 메모리 사용량, GPU -> CPU로 변경되어 비용이 10배 차이나게 됨.
  3. 배포 호환성: PyTorch모델은 PyTorch + Python환경에서 동작하는 반면, ONNX는 C++, JavaScript에서도 실행되며 브라우저에서도 돌아감. 
  > 실시간 서비스에서 빠른 응답, 더 많은 요청 처리, GPU 없이 모델 배포가 가능하며, 정확도가 유지됨.
  4. ONNX vs TensorRT vs OpenVINO: 동일한 목적으로 사용되지만, 타겟 하드웨어가 다르다. (모든 플랫폼 vs NVIDIA GPU 전용 vs intel CPU) 따라서 가장 범용적인 것은 ONNX Runtime이다.


## 오늘의 도전 과제와 해결 방법
- 학습 데이터 생성 후 학습: GPT를 통해 5000개의 데이터 셋 확보(5000개 데이터 자동화 파이프라인 구축.) -> Attention layer 7층부터 파인튜닝 -> 결과 확인.
- 추론 최적화 방법 공부하기: PyTorch(최적화 없음) vs ONNX vs TensorRT vs OpenVINO 벤치마크를 통해 비교. Latency, Throughput, 메모리 사용량, 모델 크기, 성능 변화(정확도 변화), 서빙 속도등을 비교할 예정임.


## 오늘의 회고
- 기존엔 데이터셋을 생성할때, 직접 GPT에게 명령하고 복사&붙여넣기하여 대량의 데이터를 생성했다. 그러나 이번엔 5000개 데이터셋 자동화 파이프라인을 구축하여 GPT가 5000개의 데이터를 생성하는 동안 다른 일을 할 수 있다는 것이 최대 장점인 듯 하다.
- 그동안 ONNX 최적화는 추론 속도 절감을 위해서 사용되는 것으로 생각하였는데, 모델 자체의 용량도 줄여주어 추론 안정화를 하는 방법인 것을 알게되었다.
- transformer 모델 구조 + 서빙과정 까지 한꺼번에 블로그에 올리려고 했는데,, 분량이 너무 많아질 것 같아서 줄여야겠다.


## 참고 자료 및 링크
