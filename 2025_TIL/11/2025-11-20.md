# TIL Template
날짜: 2025-11-20

## 스크럼
- 학습 목표 1 : BERT 모델 구현하기


## 새로 배운 내용
### 주제 1: transformer 모델 구현하기
- **BERT 모델의 원리 이해하기** :
1. 양방향 표현: 단방향 표현은 이전 토큰값만 사용하는 반면, 양방향 표현은 추론할 인덱스의 토큰값을 제외하고 모든 토큰을 고려하는 방식이다.
2. 자기지도학습(Encoder-only): 입력데이터의 일부를 가려(마스킹) 데이터로 학습한다. 
   Encoder-only vs Decoder-only: 인코더는 문장의 이해에 특화되어있으며, Decoder는 문장의 해석에 특화되어 있다. GPT, Gemma, hyperclovax, kanana등은 문장 해석에 특화된 Decoder-only구조이고, BERT는 Encoder-only구조이다.
- **BERT 모델 구현하기**: Encoder-only 구조로, 자연어 분류에 특화된 BERT 모델을 활용하여 MBTI 16종 분류하기 실험을 해보고자 한다. 
  BERT모델은 


## 오늘의 도전 과제와 해결 방법
- transformer 모델을 직접 구현하고, 각 layer의 텐서를 추적하여 이해할 수 있다: 



## 오늘의 회고
- 
- 


## 참고 자료 및 링크
- [링크 제목](URL)
- [링크 제목](URL)