# TIL Template
날짜: 2025-11-27

## 스크럼
- 학습 목표 1 : CS 공부하기(AI 서빙에 필요한 CS 지식 공부하기)
- 학습 목표 2 : 파인튜닝 진행하기.
- 학습 목표:3 : ONNX 추론 최적화(CPU환경 실행 속도 비교)하기


## 새로 배운 내용
### 주제 1: CS 공부하기(AI 서빙에 필요한 CS 지식 공부하기)
- **PyTorch의 실행과정(Forward 1번 돌 때 실제 내부 흐름)**:
  1. Python 코드를 실행한다.
  2. 디바이스/언어/런타임 경계 전환이 일어난다. 텐서 연산이 호출되면 Python레벨에서 텐서 연산 호출 -> PyTorch가 C++ backend (ATen)에 연산을 요청 -> C++호출 -> GPU, CPU 연산 동작을 반복하며 forward 과정이 끝날 때까지 Python과 C++,CUDA를 왕복(컨텍스트 스위칭)한다. Python이 모든 연산을 하나하나 호출하는게 포인트. 컨텍스트 스위칭을 하면 다른 환경으로 이동하기 위해 필요한 준비과정 + 주소/상태 저장/복구 비용이 든다. 
    - Python -> C++ : Python stack -> C++ native stack으로 전환
    - C++ -> CUDA Kernel(GPU): C++에서 CUDA kernel launch, CUDA driver가 GPU에 명령 전달, GPU가 커널을 실행, 다시 C++로 돌아옴의 과정을 거치며 CPU <-> GPU IO 호출을 진행하며 대규모 context switch가 발생한다. GPU launch 오버헤드가 존재한다. 
    - CUDA -> C++ -> Python 복귀: 연산이 끝나면 Python 객체(Tensor)로 포장하기위한 과정을 거침.
    -> 이렇게 왔다갔다 하는 사이에 중간텐서(gradient 계산에 필요할 수 있는 메모리)가 메모리에 누적됨. 
    ```
    중간 텐서란?
    conv_out = Conv(x)
    bn_out   = BatchNorm(conv_out)
    relu_out = ReLU(bn_out)
    ```
    > Conv -> BN -> ReLU를 연산할때, 각 과정의 텐서를 모두 저장한다.
    > 이 저장된 텐서를 중간 텐서로 부르며, ONNX는 fusion(세개의 연산을 하나로 합침)을 통해 FusedConvBNReLU를 진행하여 이런 중간 텐서를 없앤다.

  3. Autograd Engine이 자동으로 "연산 기록 테이프"를 만든다.
  4. 연산마다 중간 텐서(intermediate tensor)가 생긴다.
  5. backward()를 호출하면 autograd 엔진이 실행됨.
  -> 위 과정마다 최적화 하기 위해 생성하는 파일이 ONNX이다.


- **ONNX 추론 최적화**: 
    - 어떤 원리로 ONNX가 추론을 최적화 할까?
        | 특징            | PyTorch (Dynamic) | ONNX/TensorRT (Static) |
        | ------------- | ----------------- | ---------------------- |
        | 그래프 생성        | 실행하면서 실시간으로 기록    | export 시 한 번 생성        |
        | 중간 텐서         | 매 연산마다 생성         | 대부분 fusion됨            |
        | 메모리 재사용       | 없음                | 적극적                    |
        | kernel fusion | 거의 없음             | 적극적                    |
        | Python 개입     | 매우 큼              | 없음                     |
        | 조건문           | 실행될 때마다 결정됨       | 고정된 경로만 존재             |
        | 유연성           | 매우 높음             | 낮음                     |
        | 최적화 가능성       | 낮음                | 매우 높음                  |

    - Fusion의 효과?(TensorRT의 핵심 기능)
        - kernel launch 횟수 감소
        - 중간 텐서 제거
        - 메모리 read/write 감소
        - 연산의 locality 증가
        - 연산 하나가 더 커지고 최적화 쉬움 -> 전반적인 속도 폭증.

    - Overhead(오버헤드)란?
        - 필요하지만 실제 계산 결과에는 기여하지 않는 추가적인 비용.
        - 오버헤드가 많다 = 실제 필요한 계산보다 쓸데없는 준비과정이 많다

- **Pytorch vs ONNX vs ONNX+TensorRT**:
    | 개념               | PyTorch | ONNX  | TensorRT |
    | ---------------- | ------- | ----- | -------- |
    | Fusion           | 거의 없음   | 약간 있음 | 매우 적극적   |
    | Overhead         | 매우 많음   | 줄어듦   | 거의 없음    |
    | Kernel Launch 횟수 | 많음      | 중간    | 최소       |
    | Python 개입        | 큼       | 없음    | 없음       |
    | 중간 텐서            | 많음      | 적음    | 거의 없음    |
    | 메모리 접근량          | 높음      | 줄어듦   | 최소화      |

> 따라서 ONNX는 **추론**시 메모리 절약 + 소요시간 감소 + 최적화 라이브러리 사용가능(범용성)으로 꼭 필요한 최적화 과정!


### 주제 2: 파인튜닝 진행하기.
- **파인튜닝 진행**:

  <DistilBert-multilingual 리소스 사용량>

    <img width="397" height="120" alt="image" src="https://github.com/user-attachments/assets/3d9f7b56-628b-43b0-b2b4-a73de6a0b48f" />

  **진행시 고려사항**: 
    - 이후 CPU환경에서 서빙으로 1~2초대를 생성하기 위해 경량모델 + 한국어 가능한 모델인 DistilBert-multilingual을 선택함. 
    - 기본적인 언어 능력이 아닌, 분류를 파인튜닝하기 위해서 Encoder의 1~2 layer는 freeze하고, 3~6layer 부터 학습을 진행함.
  
  **진행 결과**:
    #### 📊 원본 DistilBERT 출력 결과 테이블

      | 욕구 | 입력 문장 | 안정감 | 인정 | 자율성 | 성취 | 소속감 | 통제감 | 유능감 | 즐거움 | 친밀감 | 공정성 |
      |------|-----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|---------|
      | 안정감 | 요즘 너무 불안하고 흔들려서 쉬고 싶어. 마음이 너무 지쳐. | 0.493 | 0.539 | 0.480 | 0.504 | 0.491 | 0.500 | 0.476 | 0.482 | 0.503 | 0.497 |
      | 인정 | 사람들에게 인정받고 싶어. 내가 노력한 걸 알아줬으면 좋겠어. | 0.491 | 0.549 | 0.478 | 0.506 | 0.497 | 0.511 | 0.482 | 0.479 | 0.511 | 0.497 |
      | 자율성 | 이건 내가 직접 선택하고 결정하고 싶어. | 0.495 | 0.545 | 0.481 | 0.503 | 0.495 | 0.504 | 0.478 | 0.487 | 0.511 | 0.496 |
      | 성취 | 이번 목표는 꼭 이루고 싶어. | 0.494 | 0.548 | 0.480 | 0.505 | 0.501 | 0.507 | 0.483 | 0.484 | 0.510 | 0.498 |
      | 소속감 | 사람들과 함께 있고 싶어. 외롭고 혼자 있는 게 싫어. | 0.490 | 0.547 | 0.479 | 0.504 | 0.495 | 0.510 | 0.483 | 0.480 | 0.513 | 0.494 |
      | 통제감 | 내 삶이 마음대로 안 되는 것 같아. 다시 통제하고 싶어. | 0.490 | 0.545 | 0.483 | 0.505 | 0.498 | 0.512 | 0.477 | 0.479 | 0.511 | 0.498 |
      | 유능감 | 나는 더 잘하고 싶어. 내 능력을 증명하고 싶어. | 0.490 | 0.542 | 0.480 | 0.505 | 0.497 | 0.510 | 0.480 | 0.476 | 0.512 | 0.500 |
      | 즐거움 | 요즘 너무 재미있고 매일 설레! | 0.492 | 0.541 | 0.470 | 0.509 | 0.488 | 0.503 | 0.490 | 0.472 | 0.498 | 0.481 |
      | 친밀감 | 따뜻한 대화를 나누고 싶어. | 0.491 | 0.550 | 0.481 | 0.504 | 0.499 | 0.507 | 0.481 | 0.484 | 0.515 | 0.497 |
      | 공정성 | 이건 정말 불공정해. 공평하게 대우받고 싶어. | 0.494 | 0.546 | 0.484 | 0.507 | 0.503 | 0.509 | 0.480 | 0.481 | 0.514 | 0.495 |

    > 어떤 문장을 입력하던지 대부분의 욕구가 0.4~0.6사이로 출력됨. -> 모델 신뢰도 낮음

    #### 📊 욕구(Needs) 예측 결과 테이블
    
      각 욕구를 강하게 나타내는 문장을 입력했을 때,  
      모델이 예측한 10차원 욕구 스코어입니다.

      | 욕구 | 입력 문장 | 안정감 | 인정 | 자율성 | 성취 | 소속감 | 통제감 | 유능감 | 즐거움 | 친밀감 | 공정성 |
      |------|-----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|---------|
      | 안정감 | 요즘 너무 불안하고 흔들려서 쉬고 싶어. 마음이 너무 지쳐. | **0.432** | 0.055 | 0.082 | 0.047 | 0.069 | 0.140 | 0.060 | 0.064 | 0.063 | 0.050 |
      | 인정 | 사람들에게 인정받고 싶어. 내가 노력한 걸 알아줬으면 좋겠어. | 0.380 | **0.776** | 0.333 | 0.654 | 0.343 | 0.318 | 0.686 | 0.334 | 0.275 | 0.191 |
      | 자율성 | 이건 내가 직접 선택하고 결정하고 싶어. 내 방식대로 하고 싶어. | 0.410 | 0.254 | **0.833** | 0.422 | 0.130 | 0.416 | 0.381 | 0.233 | 0.118 | 0.082 |
      | 성취 | 이번 목표는 꼭 이루고 싶어. 성공하고 싶은 마음뿐이야. | 0.379 | 0.345 | 0.495 | **0.815** | 0.149 | 0.307 | 0.711 | 0.416 | 0.156 | 0.089 |
      | 소속감 | 사람들과 함께 있고 싶어. 외롭고 혼자 있는 게 싫어. | 0.549 | 0.248 | 0.120 | 0.064 | **0.428** | 0.048 | 0.091 | 0.290 | 0.582 | 0.041 |
      | 통제감 | 요즘 내 삶이 마음대로 안 되는 것 같아. 다시 통제하고 싶어. | 0.466 | 0.149 | 0.565 | 0.299 | 0.104 | **0.787** | 0.295 | 0.152 | 0.073 | 0.077 |
      | 유능감 | 나는 더 잘하고 싶어. 내 능력을 증명하고 싶은 마음이 커. | 0.391 | 0.320 | 0.557 | 0.714 | 0.135 | 0.429 | **0.716** | 0.291 | 0.140 | 0.085 |
      | 즐거움 | 요즘 너무 재미있고 매일매일 설레! 즐거운 일이 많아. | 0.368 | 0.266 | 0.272 | 0.347 | 0.286 | 0.106 | 0.340 | **0.868** | 0.377 | 0.049 |
      | 친밀감 | 누군가와 깊이 연결되고 싶어. 따뜻한 대화를 나누고 싶어. | 0.498 | 0.597 | 0.453 | 0.483 | 0.484 | 0.383 | 0.507 | 0.403 | **0.592** | 0.227 |
      | 공정성 | 이건 정말 불공정해. 공평하게 대우받고 싶어. | 0.207 | 0.177 | 0.095 | 0.101 | 0.112 | 0.196 | 0.155 | 0.041 | 0.062 | **0.815** |

      > 주요 욕구는 높게, 상관 없는 욕구는 낮게 나옴 -> 신뢰도 높음.

      ## F1-score

      Threshold = 0.5: 각 라벨당 0.5 이상이면 1, 0.5이하이면 0으로 계산한 결과

      | 모델                  | Micro F1(전체 중 맞춘 비율) | Macro F1(각 class 별 F1의 평균, 가장 공정한 지표) |
      | ------------------- | -------- | -------- |
      | **원본 DistilBERT**   | 0.3176   | 0.1727   |
      | **파인튜닝 DistilBERT** | 0.8302(**2.6배 상승**)   | 0.8267(**4.8배 상승**)   |

### 주제 3:  ONNX 추론 최적화(CPU환경 실행 속도 비교)하기.
  #### DistilBERT → ONNX 변환 결과 요약

     1. 출력 일관성(Output Consistency)

      PyTorch와 ONNX Runtime의 출력 logits 비교:

      PyTorch logits:
      [-0.9236747, -2.7737150, -2.6038120, -2.6397200, -2.9590610, -1.9810871, -2.3947875, -2.2435064, -2.8706200, -2.9806132]

      ONNX Runtime logits:
      [-0.9236744, -2.7737153, -2.6038117, -2.6397202, -2.9590604, -1.9810865, -2.3947868, -2.2435074, -2.8706203, -2.9806104]

      Difference:
      [-2.98e-07, 2.38e-07, -2.38e-07, 2.38e-07, -4.76e-07, -5.96e-07, -7.15e-07, 9.53e-07, 2.38e-07, -2.86e-06]

      결론:
      - 두 모델 출력 차이는 1e-7 ~ 1e-6 수준의 부동소수점 오차.
      - ONNX 변환은 정확도 손실 없이 완벽하게 성공함.


     2. CPU 추론 속도 비교

      | 모델 | 환경 | Latency (ms) | 비고 |
      |------|--------|----------------|------|
      | PyTorch (CPU) | CPUExecutionProvider | 263.123 ms | PyTorch 기본 추론 |
      | ONNX Runtime (CPU) | CPUExecutionProvider | 164.577 ms | 최적화된 ONNX 추론 |

      결론:
      - ONNX Runtime이 PyTorch 대비 약 1.60배 빠름.
      - DistilBERT처럼 경량 모델은 CPU 최적화의 이점이 큼.


     3. 종합 평가

      | 항목 | 평가 |
      |------|------|
      | ONNX 변환 성공 여부 | 성공 |
      | 출력 유사도 | 1e-7 수준 (사실상 동일) |
      | CPU 속도 향상 | 약 1.6배 |
      | GPU 속도 비교 | Colab의 ONNX GPU provider 미지원으로 CPU 기준 평가 |
      | 서비스 적합성 | 높음 (특히 Cloud Run CPU 환경에 적합) |


     최종 결론

      - DistilBERT 욕구 분석 모델은 ONNX로 완전히 동일하게 변환되었으며,  
      - CPU 환경에서는 PyTorch 대비 약 1.6배 빠르게 동작한다.  
      - 정확도 손실 없음 + 속도 향상 → 실서비스 배포에 매우 적합한 상태다.




## 오늘의 도전 과제와 해결 방법
- **추론시 ONNX 최적화가 필요한 이유에 대해 이해한다**: PyTorch의 동작 과정을 통해 ONNX가 개선한 사항을 확인한다.
- **파인튜닝 성공시키기**: 저번 파인튜닝은 풀 파인튜닝으로 타깃 욕구가 아닌 다른 욕구들도 높게 측정되어 모델의 신뢰성이 낮았다. 이번엔 부분적인 파인튜닝을 진행하여 pretrained 모델의 기본성능을 저해시키지 않는 방향으로 진행하려고 한다.


## 오늘의 회고
- 파인튜닝은 역시 pretrained 모델에 partial finetuning을 하거나, LoRA 파인튜닝을 하는것이 기본 성능을 유지하면서 전문성을 높이는 것 같다.


## 참고 자료 및 링크
- [링크 제목](URL)
- [링크 제목](URL)