# TIL Template
날짜: 2025-11-27

## 스크럼
- 학습 목표 1 : CS 공부하기(AI 서빙에 필요한 CS 지식 공부하기)
- 학습 목표 2 : 파인튜닝 진행하기.


## 새로 배운 내용
### 주제 1: CS 공부하기(AI 서빙에 필요한 CS 지식 공부하기)
- **PyTorch의 실행과정(Forward 1번 돌 때 실제 내부 흐름)**:
  1. Python 코드를 실행한다.
  2. 디바이스/언어/런타임 경계 전환이 일어난다. 텐서 연산이 호출되면 Python레벨에서 텐서 연산 호출 -> PyTorch가 C++ backend (ATen)에 연산을 요청 -> C++호출 -> GPU, CPU 연산 동작을 반복하며 forward 과정이 끝날 때까지 Python과 C++,CUDA를 왕복(컨텍스트 스위칭)한다. Python이 모든 연산을 하나하나 호출하는게 포인트. 컨텍스트 스위칭을 하면 다른 환경으로 이동하기 위해 필요한 준비과정 + 주소/상태 저장/복구 비용이 든다. 
    - Python -> C++ : Python stack -> C++ native stack으로 전환
    - C++ -> CUDA Kernel(GPU): C++에서 CUDA kernel launch, CUDA driver가 GPU에 명령 전달, GPU가 커널을 실행, 다시 C++로 돌아옴의 과정을 거치며 CPU <-> GPU IO 호출을 진행하며 대규모 context switch가 발생한다. GPU launch 오버헤드가 존재한다. 
    - CUDA -> C++ -> Python 복귀: 연산이 끝나면 Python 객체(Tensor)로 포장하기위한 과정을 거침.
    -> 이렇게 왔다갔다 하는 사이에 중간텐서(gradient 계산에 필요할 수 있는 메모리)가 메모리에 누적됨. 
    ```
    중간 텐서란?
    conv_out = Conv(x)
    bn_out   = BatchNorm(conv_out)
    relu_out = ReLU(bn_out)
    ```
    > Conv -> BN -> ReLU를 연산할때, 각 과정의 텐서를 모두 저장한다.
    > 이 저장된 텐서를 중간 텐서로 부르며, ONNX는 fusion(세개의 연산을 하나로 합침)을 통해 FusedConvBNReLU를 진행하여 이런 중간 텐서를 없앤다.

  3. Autograd Engine이 자동으로 "연산 기록 테이프"를 만든다.
  4. 연산마다 중간 텐서(intermediate tensor)가 생긴다.
  5. backward()를 호출하면 autograd 엔진이 실행됨.
  -> 위 과정마다 최적화 하기 위해 생성하는 파일이 ONNX이다.


- **ONNX 추론 최적화**: 
    - 어떤 원리로 ONNX가 추론을 최적화 할까?
        | 특징            | PyTorch (Dynamic) | ONNX/TensorRT (Static) |
        | ------------- | ----------------- | ---------------------- |
        | 그래프 생성        | 실행하면서 실시간으로 기록    | export 시 한 번 생성        |
        | 중간 텐서         | 매 연산마다 생성         | 대부분 fusion됨            |
        | 메모리 재사용       | 없음                | 적극적                    |
        | kernel fusion | 거의 없음             | 적극적                    |
        | Python 개입     | 매우 큼              | 없음                     |
        | 조건문           | 실행될 때마다 결정됨       | 고정된 경로만 존재             |
        | 유연성           | 매우 높음             | 낮음                     |
        | 최적화 가능성       | 낮음                | 매우 높음                  |

    - Fusion의 효과?(TensorRT의 핵심 기능)
        - kernel launch 횟수 감소
        - 중간 텐서 제거
        - 메모리 read/write 감소
        - 연산의 locality 증가
        - 연산 하나가 더 커지고 최적화 쉬움 -> 전반적인 속도 폭증.

    - Overhead(오버헤드)란?
        - 필요하지만 실제 계산 결과에는 기여하지 않는 추가적인 비용.
        - 오버헤드가 많다 = 실제 필요한 계산보다 쓸데없는 준비과정이 많다

- **Pytorch vs ONNX vs ONNX+TensorRT**:
    | 개념               | PyTorch | ONNX  | TensorRT |
    | ---------------- | ------- | ----- | -------- |
    | Fusion           | 거의 없음   | 약간 있음 | 매우 적극적   |
    | Overhead         | 매우 많음   | 줄어듦   | 거의 없음    |
    | Kernel Launch 횟수 | 많음      | 중간    | 최소       |
    | Python 개입        | 큼       | 없음    | 없음       |
    | 중간 텐서            | 많음      | 적음    | 거의 없음    |
    | 메모리 접근량          | 높음      | 줄어듦   | 최소화      |

> 따라서 ONNX는 **추론**시 메모리 절약 + 소요시간 감소 + 최적화 라이브러리 사용가능(범용성)으로 꼭 필요한 최적화 과정!


### 주제 2: 
-


## 오늘의 도전 과제와 해결 방법
- **추론시 ONNX 최적화가 필요한 이유에 대해 이해한다**: PyTorch의 동작 과정을 통해 ONNX가 개선한 사항을 확인한다.


## 오늘의 회고
- 


## 참고 자료 및 링크
- [링크 제목](URL)
- [링크 제목](URL)