# TIL Template
날짜: 2025-11-17

## 스크럼
- 학습 목표 1 : 오버피팅 제어하기(모델 구조적으로)


## 새로 배운 내용
### 주제 1: 오버피팅 제어하기(모델 구조적으로)
- **오버피팅 발생원인 해석(구조적 관점)** : L2 Regularization, Dropout은 모두 모델의 복잡도를 낮추는 방법이다. 오버피팅은 과하게 세부적인 영역까지 모델이 외우는 현상으로, 모델의 복잡도를 낮춰 해결 할 수 있다.

* 오버피팅 제어 전
<img width="500" height="240" alt="image" src="https://github.com/user-attachments/assets/d9dbd540-8f6a-41db-9ced-37830a02626f" />

* L2 Regularization, Dropout 적용 후
<img width="500" height="240" alt="image" src="https://github.com/user-attachments/assets/48dbeb4e-bf82-4282-9754-992e7457e9b1" />


## 오늘의 도전 과제와 해결 방법
- L2 Regularization, Dropout을 통해서도 오버피팅 제어가 가능한지 확인: Train과 Validation accuracy의 차이가 줄어들었지만, 여전히 오버피팅 발생함.


## 오늘의 회고
- layer를 통한 오버피팅의 제어가 근본적인 해결방법일지는 의문이다.

