# TIL Template
날짜: 2025-11-29

## 스크럼
- 학습 목표 1 : DistilBERT 최적화 전략 추가.


## 새로 배운 내용
### 주제 1: DistilBERT 최적화 전략 추가.
- **TensorRT vs OpenVINO**:
    - 두 방법은 환경에 따라 다른 최적화 방식을 보인다. TensorRT는 NVIDIA GPU, OpenVINO는 Intel CPU에서의 연산을 최적화 한다. 더 공격적인 fusion, 기타 다양한 메모리 관리등의 최적화 기술이 추가된다.
    - ONNX + ONNX Runtime과 호환된다는 공통점이 있다.
    - Colab환경에서는 Intel과 AMD가 랜덤으로 할당되기 때문에, OpenVINO를 안정적으로 실험하기 위해서는 로컬환경이 필요하다.

    | 기술 | 역할(Role) | 기대되는 속도 향상   | 용량 최적화 효과  |
    | -- | -- | -- | -- |
    | **ONNX**               | - PyTorch/TensorFlow를 표준 포맷으로 변환<br>- 연산 그래프(graph)를 정적으로 변환<br>- Graph Optimization(Level 1~3)으로 노드 수 감소 및 연산 단순화                        | **기본 PyTorch 대비 10~40% 가속**<br>(그래프 최적화 효과)     | 모델 구조가 효율적으로 재배치되지만<br>**용량 변화는 거의 없음** (수 MB 감소 수준)                    |
    | **ONNX Runtime (ORT)** | - ONNX 모델을 실행하는 고속 추론 엔진<br>- 멀티스레드 병렬화 (intra/inter op)<br>- 연산자 fusion, 최적 커널 자동 선택<br>- CPU/GPU Execution Provider 선택 가능               | **ONNX 대비 추가 10~70% 가속**<br>CPU 추론에서 가장 체감 큼    | 용량은 ONNX와 동일<br>하지만 **메모리 사용량 감소**(불필요한 텐서 제거)                          |
    | **OpenVINO (ORT EP)**  | - Intel 하드웨어 특화 최적화 엔진<br>- AVX-512/VNNI 기반 벡터연산 가속<br>- Low Precision(INT8/FP16) 최적화<br>- aggressive operator fusion & memory layout 최적화 | **ORT 대비 +20~50% 추가 가속**<br>※ Intel CPU에서 성능 최고 | 양자화(INT8) 적용 시<br>**모델 크기 최대 3~4배 감소**<br>(weight-only quantization 기준) |

    > ONNX + ONNX Runtime(ORT) + OpenVINO(ORT EP) 를 통해 CPU 환경에서 모델 최적화를 진행 예정.


- **양자화 기법**:
    - **PTQ vs QAT vs Weight-only**:

    | 기법 | 주요 목적 | 예상 속도 향상 | 예상 용량 감소 | 특징 요약 |
    | -- | -- | -- | -- | --|
    | **PTQ (Post-Training Quantization, INT8)**  | 학습 없이 INT8로 빠르게 최적화 | **1.5× ~ 2.5×**            | **약 3~4× 감소 (INT8)**  | 가장 많이 쓰는 양자화. 빠르고 안정적.                |
    | **QAT (Quantization-Aware Training, INT8)** | 정확도 유지하며 INT8 최적화   | **≈2× (PTQ와 비슷하거나 약간 우세)** | **INT8 기준 3~4×**      | 학습 단계에서 양자화 시뮬레이션. 정확도 손실 최소.         |
    | **INT4 Weight-only 기법들 (GPTQ · AWQ · OWQ)** | INT4로 극한 용량/속도 절감   | **1.2× ~ 2.2×**            | **4× ~ 8× 감소 (INT4)** | 가중치만 INT4로 내리는 방식. INT8보다 압축력이 매우 높음. |

    - **Weight-only 양자화(GPTQ vs AWQ vs OWQ)**:
  
    | 기법 | 주요 목적 | 정확도 안정성 | 속도 향상 | 용량 감소 | 기술적 특징 |
    | -- | -- | -- | -- | -- | -- |
    | **GPTQ** | 기본적인 INT4 weight-only PTQ  | 중간      | **1.2× ~ 2×**   | **4× ~ 8×** | 가장 단순하고 빠름. 일부 레이어에서 정확도 흔들릴 수 있음.       |
    | **AWQ**  | Outlier(특이값) 보호 → 안정적 INT4 | 높음      | **1.2× ~ 2×**   | **4× ~ 8×** | GPTQ보다 안정적. 중요한 weight를 보호하는 구조.         |
    | **OWQ**  | 스케일 최적화 기반 INT4 최고 품질      | 매우 높음   | **1.3× ~ 2.2×** | **4× ~ 8×** | GPTQ/AWQ보다 INT4 재현성 및 품질이 가장 우수. 최신 트렌드. |

    > PTQ + OWQ를 통해 최종 int4로 최적화를 진행 예정.

## 오늘의 도전 과제와 해결 방법
- 현재 task에 가장 적합한 최적화 기법을 찾는다. : 추론 속도, 모델 용량과 관련된 최적화 기법을 조사하여 가장 빠르고 가장 적은 용량의 모델로 최적화 할 수 있는 방법을 고안한다.


## 오늘의 회고
- ONNX 최적화만 있는 줄 알았는데, OWQ까지 최적화가 가능하다는 사실을 알게 되어 이번 프로젝트에서는 두개의 양자화 기법을 사용해야겠다.
- 모델 속도 최적화를 진행 후에는 직접 모델을 서빙하는 전략을 고민해야겠다.
