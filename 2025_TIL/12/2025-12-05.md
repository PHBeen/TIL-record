# TIL Template
날짜: 2025-12-05

## 스크럼
- 학습 목표 1 : CogVideoX 다시 공부하기..


## 새로 배운 내용
### 주제 1: CogVideoX 다시 공부하기.
- **텍스트 임베딩**: 

    | 단계                         | 무슨 과정인가                  | 출력 텐서             | 내부 연산                        |
    | -------------------------- | ------------------------ | ----------------- | ---------------------------- |
    | **1. 토큰화(Tokenization)**   | 문장을 정수 ID로 변환            | `[B, L]` (L ≤ 77) | BPE 토크나이저                    |
    | **2. Token Embedding**     | 각 토큰 ID를 벡터로 변환          | `[B, L, D]`       | Lookup table (Embedding)     |
    | **3. Position Embedding**  | 토큰 위치 정보 추가              | `[B, L, D]`       | Learnable position embedding |
    | **4. Transformer Encoder** | 문장의 의미 정제                | `[B, L, D]`       | Self-Attention + FFN         |
    | **5. Projection Layer**    | CogVideoX hidden dim에 맞춤 | `[B, L, D_model]` | Linear projection            |

    layer 0~3: 문장 기본 구조 파악
    layer 4~10: 단어 사이 관계·의미 정렬
    layer 10~20: 장면 구성 정보 추출 (예: glowing, rabbit, run 같은 시각적 키워드 강화)
    layer 20~23: 최종 비디오 생성에 최적화된 의미 벡터로 압축

    <img width="500" height="300" alt="text_encoder_layer_0" src="https://github.com/user-attachments/assets/20d6fd09-abc0-42f3-9e1a-8db29e617739" />
    <img width="500" height="300" alt="text_encoder_layer_23" src="https://github.com/user-attachments/assets/441eb082-fae3-46f7-a9c8-360c5653e698" />


- **이미지 임베딩**:

    | 단계                       | 무슨 과정인가                      | 출력 텐서                         | 내부 연산                 |
    | ------------------------ | ---------------------------- | ----------------------------- | --------------------- |
    | **1. 이미지/프레임 입력**        | RGB 이미지/프레임 준비               | `[B, F, 3, H, W]`         | 없음                    |
    | **2. 3D VAE Encoder**    | 비디오/이미지를 latent로 압축          | `[B, F, C, H', W']`           | Conv3D + Downsampling |
    | **3. Patch 분할**          | latent를 여러 패치로 자름          | `[B, N_patches, patch_vec]`   | 패치 단위로 slice          |
    | **4. Flatten**           | 패치를 1D 벡터로 변환                | `[B, N_patches, C * ph * pw]` | reshape/flatten       |
    | **5. Linear Projection** | Transformer의 hidden dim으로 맞춤 | `[B, N_tokens, D_model]`      | Linear layer          |

    <img width="500" height="300" alt="vae_encoder_layer_0" src="https://github.com/user-attachments/assets/4166eb1b-844b-4f1e-9fc2-680a7dc5f351" />
    <img width="500" height="300" alt="vae_encoder_layer_31" src="https://github.com/user-attachments/assets/5ef27b12-8c5f-4e91-b62f-519c06fa11dc" />

- **transformer 입력**:

    | 구성                          | 출력 텐서                            | 설명                  |
    | --------------------------- | -------------------------------- | ------------------- |
    | **텍스트 토큰 시퀀스**              | `[B, L_text, D_model]`           | 문장 정보               |
    | **비디오 latent 토큰 시퀀스**       | `[B, N_video, D_model]`          | 영상 구조·모션 정보         |
    | **Concat 후 Transformer 입력** | `[B, L_text + N_video, D_model]` | DiT가 처리하는 최종 토큰 시퀀스 |



## 오늘의 도전 과제와 해결 방법
- 최적화 전략을 이해하기 위해.. CogVideoX의 내부를 좀더 완벽히 이해해야겠다.


## 오늘의 회고
-  
