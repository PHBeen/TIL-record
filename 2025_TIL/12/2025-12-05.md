# TIL Template
날짜: 2025-12-05

## 스크럼
- 학습 목표 1 : CogVideoX 다시 공부하기.
- 학습 목표 2: 해상도 512x288 영상 생성하기


## 새로 배운 내용
### 주제 1: CogVideoX 다시 공부하기.
- **텍스트 임베딩**: 

    | 단계                         | 무슨 과정인가                  | 출력 텐서             | 내부 연산                        |
    | -------------------------- | ------------------------ | ----------------- | ---------------------------- |
    | **1. 토큰화(Tokenization)**   | 문장을 정수 ID로 변환            | `[B, L]` (L ≤ 77) | BPE 토크나이저                    |
    | **2. Token Embedding**     | 각 토큰 ID를 벡터로 변환          | `[B, L, D]`       | Lookup table (Embedding)     |
    | **3. Position Embedding**  | 토큰 위치 정보 추가              | `[B, L, D]`       | Learnable position embedding |
    | **4. Transformer Encoder** | 문장의 의미 정제                | `[B, L, D]`       | Self-Attention + FFN         |
    | **5. Projection Layer**    | CogVideoX hidden dim에 맞춤 | `[B, L, D_model]` | Linear projection            |

    layer 0~3: 문장 기본 구조 파악
    layer 4~10: 단어 사이 관계·의미 정렬
    layer 10~20: 장면 구성 정보 추출 (예: glowing, rabbit, run 같은 시각적 키워드 강화)
    layer 20~23: 최종 비디오 생성에 최적화된 의미 벡터로 압축

    <img width="500" height="300" alt="text_encoder_layer_0" src="https://github.com/user-attachments/assets/20d6fd09-abc0-42f3-9e1a-8db29e617739" />
    <img width="500" height="300" alt="text_encoder_layer_23" src="https://github.com/user-attachments/assets/441eb082-fae3-46f7-a9c8-360c5653e698" />


- **이미지 임베딩**:

    | 단계                       | 무슨 과정인가                      | 출력 텐서                         | 내부 연산                 |
    | ------------------------ | ---------------------------- | ----------------------------- | --------------------- |
    | **1. 이미지/프레임 입력**        | RGB 이미지/프레임 준비               | `[B, F, 3, H, W]`         | 없음                    |
    | **2. 3D VAE Encoder**    | 비디오/이미지를 latent로 압축          | `[B, F, C, H', W']`           | Conv3D + Downsampling |
    | **3. Patch 분할**          | latent를 여러 패치로 자름          | `[B, N_patches, patch_vec]`   | 패치 단위로 slice          |
    | **4. Flatten**           | 패치를 1D 벡터로 변환                | `[B, N_patches, C * ph * pw]` | reshape/flatten       |
    | **5. Linear Projection** | Transformer의 hidden dim으로 맞춤 | `[B, N_tokens, D_model]`      | Linear layer          |

    <img width="500" height="300" alt="vae_encoder_layer_0" src="https://github.com/user-attachments/assets/4166eb1b-844b-4f1e-9fc2-680a7dc5f351" />
    <img width="500" height="300" alt="vae_encoder_layer_31" src="https://github.com/user-attachments/assets/5ef27b12-8c5f-4e91-b62f-519c06fa11dc" />

- **transformer 입력**:

    | 구성                          | 출력 텐서                            | 설명                  |
    | --------------------------- | -------------------------------- | ------------------- |
    | **텍스트 토큰 시퀀스**              | `[B, L_text, D_model]`           | 문장 정보               |
    | **비디오 latent 토큰 시퀀스**       | `[B, N_video, D_model]`          | 영상 구조·모션 정보         |
    | **Concat 후 Transformer 입력** | `[B, L_text + N_video, D_model]` | DiT가 처리하는 최종 토큰 시퀀스 |

- **3D Decoder**: 

    | 단계                         | 무슨 과정인가                         | 출력 텐서                    | 내부 연산                              |
    | -------------------------- | ------------------------------- | ------------------------ | ---------------------------------- |
    | **1. Latent 입력**           | 3D VAE가 만든 저해상도 latent를 입력받음    | `[B, C=16, F=1, 60, 90]` | 없음                                 |
    | **2. 채널 확장(CausalConv3D)** | latent를 고차원 feature space로 확장   | `[B, 512, 1, 60, 90]`    | Conv3D(16→512)                     |
    | **3. Mid ResNet Blocks**   | 해상도는 유지한 채 장면의 구조·조명·스타일 정보 정교화 | `[B, 512, 1, 60, 90]`    | (ResNet3D + Norm + SiLU) × 여러번     |
    | **4. 업샘플 단계 1**            | 해상도 업샘플: 60×90 → **120×180**    | `[B, 512, 1, 120, 180]`  | Upsample3D(scale=2) + ResNet3D     |
    | **5. 업샘플 단계 2**            | 해상도 업샘플: 120×180 → **240×360**  | `[B, 256, 1, 240, 360]`  | Upsample3D + ResNet3D (채널 512→256) |
    | **6. 업샘플 단계 3**            | 해상도 업샘플: 240×360 → **480×720**  | `[B, 256, 1, 480, 720]`  | Upsample3D + ResNet3D              |
    | **7. 채널 정리 블록**            | RGB 변환을 준비하기 위해 feature 축소      | `[B, 128, 1, 480, 720]`  | Conv3D(256→128) + Norm             |
    | **8. 최종 RGB Conv3D**       | 최종 영상을 생성하는 단계                  | `[B, 3, 1, 480, 720]`    | Conv3D(128→3)                      |


-> 해상도는 60:90으로 출력해야함. 가장 낮은 해상도는 512x288이며, 해당 해상도의 영상 품질(표현력)이 좋은 지 확인 후 선택 예정.

### 주제 2: 해상도 512x288 영상 생성하기

- 프롬프트 조건: 늑대 두마리가 있을 것. 디즈니 공간감을 가질 것.


https://github.com/user-attachments/assets/19fdb640-7634-4f12-83ea-1024577946dc


-> 품질이 아쉬우나(아래 부분에 찢어지는 부분이 보임), 표현력은 프롬프트를 잘 반영한 것으로 보인다. 
-> 업스케일링이 필수적으로 보인다. 

## 오늘의 도전 과제와 해결 방법
- 최적화 전략을 이해하기 위해.. CogVideoX의 내부를 좀더 완벽히 이해해야겠다.


## 오늘의 회고
-  드디어 영상을 생성해보았다. 다만, 추론에 GPU A100은 필수로 사용되며, 50GB이상 필요하다. 모델을 최적화 하는 방향으로 다시 고려해야겠다.
