# TIL Template
날짜: 2025-12-02

## 스크럼
- 학습 목표 1 : 유니티 게임 제작 입문 책 읽기
- 학습 목표 2 : Zeroscope v2 XL vs CogVideoX 2B로 모델 선정하기.


## 새로 배운 내용
### 주제 1: 유니티 게임 제작 입문 책 읽기
- **유니티란**: 모바일 게임, 데스크탑 게임을 비롯해 AR, VR을 만드는데에 사용되는 툴. 
    DiT기반 영상 제작시 게임 그래픽 기반으로 생성하기 위해 공부하고자 함.

- **유니티 공부 우선순위 조정 필요**:  직접 배경을 그려보고, 다양한 빛을 넣어보며 그래픽을 공부하고자 하였으나,, 유니티는 그래픽 디자인 툴 보다는 캐릭터의 움직임, 장치와의 연결, 동작 등에 초점이 맞춰져 있어, 공부량이 방대하다. DiT 구조를 먼저 공부 후에 필요시 유니티를 추가로 공부할 예정이다.

### 주제 2: Zeroscope v2 XL vs CogVideoX 2B로 모델 선정하기.
- **Zeroscope v2 XL vs CogVideoX 2B**:

Zeroscope v2 XL은 Diffusion + Transformer의 하이브리드 모델이며, Diffusion특유의 최적화 문제를 가진다. 이에 추가 경량화 및 속도 최적화가 불가할 것으로 생각이 들며, CogVideoX 2B로 진행하고자 한다. 

| 항목                   | **Zeroscope v2 XL**             | **CogVideoX 2B (최경량)**                 |
| -------------------- | ------------------------------- | -------------------------------------- |
| **모델 종류**            | Video Diffusion + Temporal UNet | 완전 Transformer 기반 Video LLM            |
| **파라미터 수**           | 약 6.7B (UNet 포함 시 ~7B+)         | 약 2B                                   |
| **모델 크기**            | ~7.5GB                          | ~18GB (FP16) / ~9GB(FP8)               |
| **최소 VRAM 요구**       | **12GB~16GB** (권장: 24GB)        | **8GB~10GB** (FP8 INT8 기준)             |
| **CPU 사용**           | 사실상 불가능 (너무 느림)                 | 2B 모델이지만 CPU 단독 추론은 사실상 불가             |
| **지원 해상도**           | 512×1024 or 576×1024            | 720p~1080p (vertical/horizontal)       |
| **생성 가능한 길이**        | 2~6초                            | 4~16초                                  |
| **프레임 수**            | 16프레임 기본                        | 32~128프레임                              |
| **속도**               | 매우 느림 (Diffusion 반복)            | 빠름 (Transformer 1-pass 또는 적은 반복)       |
| **최적화 가능 여부**        | △ 제한적 (ONNX, xFormers만)         | ◎ 매우 좋음 (ONNX, TensorRT, AWQ, GPTQ 가능) |
| **양자화 가능 여부**        | △ FP16→FP8 정도는 가능, INT8 불안정     | ◎ INT8/FP8 양자화 잘됨 (안정적)                |
| **오픈소스 여부**          | ✔ 완전 오픈                         | ✔ 완전 오픈 (허깅페이스 공개)                     |
| **논문 여부**            | X (커뮤니티 기반)                     | O (공식 논문 발표됨)                          |
| **ControlNet 지원**    | △ 거의 불가능                        | O 일부 버전에서 conditioning 지원              |
| **일관성(Consistency)** | 낮음 (캐릭터 유지 어려움)                 | 매우 높음 (Transformer 특성)                 |
| **카메라 무빙**           | 잘됨                              | 매우 자연스러움                               |
| **학습/파인튜닝**          | 매우 어려움                          | 상대적으로 쉬움 (LoRA 지원 예정/일부 지원)            |
| **사용 난이도**           | 쉬움                              | 중간 (설치 무거움)                            |
| **실사용 적합성**          | 배경 영상 생성                        | 동화·드라마·캐릭터 중심 영상에 적합                   |

- **CogVideoX 2B 모델 구조 이해하기**:
    1. 영상의 텐서: (frame, width, Hight, channel)
    2. 3D VAE: video latent로 압축. 연산텐서를 몇배 압축 가능함. (frame/4, width/4, Hight/4, channel/4) -> 4배 압축 
    3. Transformer 연산의 패치 단위 토큰 생성(2프레임씩, 2픽셀씩, 2픽셀씩, channel)을 묶어서 한개의 patch로 생성함. 이를 projection layer에서 hidden dim차원만큼 변환함 -> 2차원 텐서가 됨.(토큰개수, dim)
    4. attention 수행
    5. 디코더: 다시 원본 텐서로 변환 -> 최종 영상 출력함.

- **의문 사항**: 
    - 패치단위로 잘라서 flatten하면 퀄리티 손상이 없을까? : transformer가 이해할 수 있는 구조로 변환한 것일 뿐, 정보의 손상은 없다. 오히려 1024로 dim이 늘어나면서, 내면의 정보를 이해하게 된다. 

- **결과**:
https://github.com/user-attachments/assets/d60ffd22-da2d-47a1-9cb9-a8affd3bf119

## 오늘의 도전 과제와 해결 방법
- 직접 CogVideoX 2B모델 구조를 이해하고, 영상을 출력해본다.: HuggingFace에 공개되어 있는 CogVideoX를 다운받아, 프롬프트 입력 후에 모델을 출력한다.


## 오늘의 회고
- 생각보다 영상 퀄리티가 너무 좋아서 놀랐다. 
