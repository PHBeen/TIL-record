# TIL Template
날짜: 2025-12-04

## 스크럼
- 학습 목표 1 : dim layer 변경 없이 FFN, transformer block 수만 줄여서 distilation 진행.
- 학습 목표 2: Distillation 대신 추론 최적화 진행하기.


## 새로 배운 내용
### 주제 1: dim layer 변경 없이 FFN, transformer block 수만 줄여서 distilation 진행
- **encoder, decoder는 원본 CogVideoX 2B 것을 그대로 써야한다. 해당 layer는 파인튜닝을 진행하기 어렵기 때문이다**: 

- **OOM 발생**: 백본모델의 학습전, latent space의 텐서 shape을 계산하기 위해 약 10GB의 GPU를 사용하고, distillation진행 과정에 teacher 모델의 추론이 필요하여 GPU의 약 10GB를 차지하게 되면서 out of memmory가 발생함. 
teacher 모델을 cpu에서, 추론하고, 추론결과를 GPU로 돌리는 등, 추가적인 메모리 관리가 필요함.

<img width="1400" height="373" alt="image" src="https://github.com/user-attachments/assets/93327c23-c7cb-455c-a204-f7630de5456d" />

- **error**: device 불일치 에러, 타입 불일치 에러가 발생. 연산이 동일한 환경에서 이루어질 수 있도록 하였으나, 지속적인 device 불일치 에러가 발생함.  -> 이는 CogVideoX 파이프라인 내부에서 to("gpu")를 사용하고 있음으로 확인함. 직접 레포내에 GPU가 하드코딩 되어있는 경우는 확인하지 못하였으나, CPU연산이 가능한 것도 확인되지 않았음. 

    -> CogVideoX 레포 내 .cuda() 또는 .to("cuda")와 같은 GPU 하드코딩은 명시적으로 없지만, 내부적으로 .to(self.weight.device) 방식이나 diffusers 기반 구조에서 GPU 전제를 깔고 동작하는 흐름이 있음. 이 때문에 CPU-only 실행은 이론적으론 가능하나, 실질적으로는 어렵다

### 주제 2: Distillation 대신 추론 최적화 진행하기.
- **적용 가능한 기술 검토하기(전체)**:

    | 방법                                | 절감 추천값               | 예상 절감율        | 품질 감소 여부             |
    | --------------------------------- | -------------------- | ------------- | -------------------- |
    | **Scheduler step 감소**             | 60 → 35              | **40% 감소**    | 매우 낮음                |
    | **Scheduler step 크게 감소**          | 60 → 24              | **60% 감소**    | 약간 감소                |
    | **Low-rank SVD 압축**               | Rank = 64~128        | **20~40% 감소** | 거의 없음                |
    | **Attention head 수 축소**           | head 32 → 24         | **10~20% 감소** | 매우 낮음                |
    | **큰 폭 head 축소**                   | head 32 → 16         | **20~35% 감소** | 약간 감소                |
    | **Token Merge(TOME)**             | token 수 10~20% 감소    | **20~30% 감소** | 거의 없음                |
    | **해상도 낮춰 생성 후 VSR 업스케일**          | 720p → 512p 생성       | **3×~8× 감소**  | 낮음(업스케일러 품질에 따라 다름)  |
    | **Latent resolution 축소**          | latent H/W 1단계 축소    | **2~4× 감소**   | 중간 (하지만 튜닝 가능)       |
    | **FFN inner_dim 축소**              | 7680 → 3840 → 2048   | **2×~4× 감소**  | 중간 |
    | **Transformer block 수 감소**        | 48 → 32 → 24 → 16    | **30~65% 감소** | 중간~높음 (감소폭 커질수록)     |
    | **KV cache reuse**                | frame-to-frame share | **10~30% 감소** | 매우 낮음                |
    | **Half-frame generation**         | 프레임 절반 생성            | **50% 감소**    | 중간                   |
    | **Graph simplification**          | reshape/permute fuse | **10~20% 감소** | 없음                   |
    | **Weight pruning (unstructured)** | 20~40% prune         | **10~30% 감소** | 낮음 (20% 이하), 중간(40%) |
    | **Text embedding caching**        | encoder skip         | **10~15% 감소** | 없음                   |
    | **CFG scale 감소**                  | 7.5 → 3.0            | **10~20% 감소** | 매우 낮음                |

- **적용 가능한 기술 검토하기(품질 하락 없음)**: CogVideoX는 최대한 리소스를 절약하는 것이 중요하기 때문에 품질이 하락 없는 내에서는 모든 기술을 적용해보고자 한다. 
    | 기술                                   | 절감 추천 값                  | **예상 메모리 절감률** | **기술 난이도** | **추천도** |
    | ------------------------------------ | --------------------- | -------------- | ---------- | ------- |
    | **1. Scheduler step 감소**             | 60 → 35                  | ★★☆☆☆ (15~25%) | ★☆☆☆☆      | ★★★★★   |
    | **2. Low-rank SVD 압축**               | Rank 64~128              | ★★★☆☆ (20~35%) | ★★★☆☆      | ★★★★★   |
    | **3. Token Merge(TOME)**             | Token 10~20% merge       | ★★★☆☆ (20~30%) | ★★☆☆☆      | ★★★★☆   |
    | **4. KV Cache Reuse**                | Frame 간 K/V 공유           | ★★☆☆☆ (10~20%) | ★★★☆☆      | ★★★★☆   |
    | **5. Graph Simplification**          | reshape/permute chain 제거 | ★★☆☆☆ (10~20%) | ★★★★☆      | ★★★☆☆   |
    | **6. Text Embedding Caching**        | 동일 prompt 시 encoder skip | ★★☆☆☆ (10~15%) | ★☆☆☆☆      | ★★★★★   |
    | **7. CFG Scale 감소**                  | 7.5 → 3.0~4.0            | ★☆☆☆☆ (5~10%)  | ★☆☆☆☆      | ★★★★☆   |
    | **8. Half-Resolution + VSR Upscale** | 720p 생성 → 512p 생성 후 VSR  | ★★★★☆ (30~50%) | ★★☆☆☆      | ★★★★☆   |
    | **9. Cross-frame latent reuse**      | 초기 latent 일부 공유          | ★★☆☆☆ (10~15%) | ★★☆☆☆      | ★★★★☆   |



## 오늘의 도전 과제와 해결 방법
- distilation으로 Compression 기술 익히기


## 오늘의 회고
- CogVideoX는 distillation 기술을 적용하기에, OOM이 발생함을 확인했다. CogVideoX는 입력 데이터가 프롬프트(텍스트), 3D 영상 두가지인 점에서 연산 메모리가 매우 부족하다. Distillation은 teacher모델(2B)의 추론과 student모델의 학습이 동시에 이루어지기 때문에, 2B모델 한개를 실행하는데에도 버거운 환경에서 두개를 돌리게 되어, OOM문제를 피할 수 없음을 확인하였다. 이에, 직접 CogVideoX의 1B 경량모델을 만드는 대신, 추론 최적화 기법을 통해 메모리 부족 문제를 해결하고자 한다.


