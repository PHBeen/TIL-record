# TIL Template
날짜: 2025-12-03

## 스크럼
- 학습 목표 1 : CogVideoX 2B XL -> CogVideoX 0.5B로 커스텀 모델 만들기(병목구간 확인, 개선)


## 새로 배운 내용
### 주제 1: CogVideoX 2B XL -> CogVideoX 0.5B로 커스텀 모델 만들기(병목구간 확인, 개선)
- **DiT 관련 논문 분석하기**: 
    - Introduction
        - DiT는 Diffusion 모델 backbone(U-Net backbone)에서 transformer backbone으로 바꾼 Text to Video model이다.
        - 기존 DiT(Diffusion Transformer) 장시간 일관되고 역동적인 장면을 생성하기엔 한계가 있었다.
        - 이에, 3D VAE, Expert Transformer, Progressive Traning Pipeline, Video 필터링+ 캡셔닝 파이프라인을 개발하여 장시간(10초) 비디오 생성이 가능하고, 영상 장면의 일관성이 높으며, 풍부한 동작 표현을 가진 영상을 생성하도록 설계하였다.

    - The CogVideoX Architecture

        <img width="400" height="500" alt="image" src="https://github.com/user-attachments/assets/dc5806a7-5bbc-4867-933d-a3e7157f7d30" />

        1. Text encdoer를 통해 text가 들어오고, LLM의 토크나이저, Token Embedding layer, positional Embedding, text encoder를 통해 텍스트 토큰으로 변경된다. (Batch, N_sequence, Dimension)
        2. 이미지는 3D VAE를 통해 latent space로 압축되고 patching을 통해 조각으로 flattnen 되며, 3D positional Embedding을 거친다. (Batch, N_image, Dimension)
        3. MM-DiT구조를 가진다. Text token(Batch, N_sequence, Dimension)과 image token(Batch, N_image, Dimension)을 concat하여 transformer 블록에 입력으로 넣는다. 이 입력은 Q,K,V가 되어 transformer 연산을 진행한다.
        4. 2B에서 transformer block은 총 48 layer이다.

- **CogVideo2B 모델 리소스 확인**: 
    
    <img width="498" height="137" alt="image" src="https://github.com/user-attachments/assets/f64c73ae-b5b0-4fbf-8936-6529c033012a" />

    > 모델 자체의 크기는 크지 않지만, 입력 데이터가 3D 데이터로 연산 과정에서 필요한 메모리가 많아 OOM을 방지하기 위해 A100을 사용해야한다.

- **CogVideo2B -> 0.5B distilation 병목구간 확인으로 경량화 실험**: 

    1. Embedding (Text)

        | Stage | Input Shape | Output Shape | Details |
        |------|-------------|--------------|---------|
        | Text Encoder Output | (B, 77, 4096) | (B, 77, 4096) | CLIP/T5 embeddings |
        | text_proj (Linear) | (B, 77, 4096) | (B, 77, 1920) | Map to transformer dim |
        | Positional Encoding | (B, 77, 1920) | (B, 77, 1920) | Adds position info |
        | Final Text Tokens | — | (B, 77, 1920) | Used as transformer input |


    2. Embedding (Image / Video Latent)

        | Stage | Input Shape | Output Shape | Details |
        |------|-------------|--------------|---------|
        | VAE Latent | (B, 16, F, 32, 32) | same | Latent video grid |
        | Conv2d Patchify (2×2) | (B, 16, F, 32, 32) | (B, T_img, 1920) | Patchify + channel expansion |
        | Reshape to Tokens | (B, T_img, 1920) | (B, T_img, 1920) | Video tokens |
        | Positional Encoding | (B, T_img, 1920) | (B, T_img, 1920) | Tubelet positional info |
        | Final Image Tokens | — | (B, T_img, 1920) | Transformer input |


    3. Transformer Block (Repeated 30 times)

        | Component | Input Shape | Output Shape | Param Size | Notes |
        |-----------|--------------|----------------|------------|--------|
        | Norm1 | (B, T, 1920) | same | small | AdaNorm style |
        | QKV Projection | (B, T, 1920) | (B, T, 1920×3) | ~11M | Linear(1920→5760) |
        | Multi-Head Attention | (B, T, 1920) | same | — | Self-attention |
        | Proj_out | (B, T, 1920) | same | ~3.6M | Linear(1920→1920) |
        | Norm2 | (B, T, 1920) | same | small | AdaNorm |
        | FFN Expand | (B, T, 1920) | (B, T, 7680) | ~14.7M | Linear(1920→7680) |
        | FFN Project | (B, T, 7680) | (B, T, 1920) | ~14.7M | Linear(7680→1920) |
        | Total/Block | — | — | ~55–60M | 30 layers → ~1.7B params |


    4. Decoder

        | Stage | Input Shape | Output Shape | Details |
        |-------|--------------|-------------|---------|
        | Transformer Output | (B, T, 1920) | same | Sequence of tokens |
        | norm_out (AdaNorm) | (B, T, 1920) | same | Time-conditioned |
        | proj_out (Linear) | (B, T, 1920) | (B, T, 64) | Noise channels |
        | Final reshaping | (B, T, 64) | latent grid | Used for diffusion step |


    5. 예상 병목 구간 (현재 vs 개선 후)

        | Component | 현재 (2B) | 개선 후 (0.5B) | 변화 |
        |-----------|-----------|----------------|--------|
        | Hidden Dim | 1920 | 960 | 4× 감소 |
        | FFN Dim | 7680 | 3840 | 4× 감소 |
        | QKV Projection | 1920→1920×3 | 960→960×3 | 4× 감소 |
        | Layers | 30 | 12–15 | 2× 감소 |
        | Total Params | ~2.0B | ~0.45–0.6B | 약 70–80% 감소 |
        | VRAM 사용 | 16–22GB | 5–7GB | 3× 감소 |

    - 따라서 Hidden Dim, FFN Dim, Layers 의 수를 줄여 파라미터를 크게 개선하고, 영상 생성에 필요한 리소스를 크게 줄이고자 한다.

- **Teacher model(2B)의 config**:

    | key                               | 값         | 의미                                 |
    | --------------------------------- | --------- | ---------------------------------- |
    | sample_frames                     | **49**    | 49프레임 영상                           |
    | sample_height                     | **60**    | latent height                      |
    | sample_width                      | **90**    | latent width                       |
    | patch_size                        | **2**     | spatial patchify 크기                |
    | text_embed_dim                    | **4096**  | text encoder dim                   |
    | max_text_seq_length               | **226**   | CLIP text token 최대 길이              |
    | temporal_compression_ratio        | **4**     | 49프레임을 49/4=12.25개 프레임으로 압축        |
    | use_learned_positional_embeddings | **False** | pos_embed 없음 → RoPE/or index-based |

    >이미지 토큰 수는 image_tokens = 49 × 30 × 45 = 49 × 1350 = 66150, 텍스트 토큰 수는 226이다.

- **distilate 결과**: 아직 확인 못했음. distilate 모델 생성에는 성공하였으나, 영상 출력 과정에서 OOM 발생.. 


## 오늘의 도전 과제와 해결 방법
- DiT와 ViT관련 모델의 내부를 파악하고 이를 블로그에 업로드하기. : DiT의 순전파&역전파 흐름을 텐서구조, 연산, heatmap을 통해 알아보기.


## 오늘의 회고
- 영상 생성은 너무 많은 리소스를 요하기 때문에 추론하기 어렵다..
